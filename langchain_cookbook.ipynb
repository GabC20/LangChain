{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **System**: Helpful background context that tell the AI what to do\n",
    "\n",
    "- **Human**: Messages that are intended to represent the user\n",
    "\n",
    "- **AI**: Messasges that shows what the AI responded with\n",
    "\n",
    "Chat is like text, but specified with a message type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"1803944467b417b9f37711da30ca0f177c69f0dbd0bd23e32e5fb5e21858ac5e\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.7, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You can try a Caprese salad with fresh tomatoes, mozzarella cheese, basil, and balsamic glaze.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([\n",
    "    SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in a short sentence.\"),\n",
    "    HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"While you're in Nice, you should also visit the Promenade des Anglais, the Old Town, and the Musée Matisse.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([\n",
    "    SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel to in a short sentence.\"),\n",
    "    HumanMessage(content=\"I like beaches, what should I travel to?\"),\n",
    "    AIMessage(content=\"You should go to Nice, France.\"),\n",
    "    HumanMessage(content=\"What else should I do while I'm there?\")\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents\n",
    "\n",
    "An object that holds a piece of text and metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"This is my document. It is full of tetx that I've gathered from other places.\", metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"This is my document. It is full of tetx that I've gathered from other places.\",\n",
    "         metadata={\n",
    "             \"my_document_id\": 234234,\n",
    "             \"my_document_source\": \"The LangChain Papers\",\n",
    "             \"my_document_create_time\": 1680013019\n",
    "         })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models - The interface to the AI brains\n",
    "\n",
    "### Language Model\n",
    "\n",
    "Text In => Text Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSaturday'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What day comes after friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model\n",
    "\n",
    "Message In => Message Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Well, have you considered walking there? It might be a bit of a long journey, but think of all the nice scenery you'll see along the way. Plus, it's great exercise!\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([\n",
    "    SystemMessage(content=\"You are an unhelpful AI bot that makes jokes at whatever the user says.\"),\n",
    "    HumanMessage(content=\"I would like to go to New York, how should I do this?\"),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embedding Model\n",
    "\n",
    "Change text into a vector. Mainly used when comparing two pieces of text together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi! It's time for the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embedding is length 1536\n",
      "Here's a sample: [-0.00020583387231454253, -0.003205398330464959, -0.0008301587076857686, -0.01946892775595188, -0.015162716619670391, 0.03127158433198929, -0.016048219054937363, -0.011687422171235085, 0.0093159731477499, -0.013513012789189816]\n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print(f\"Your embedding is length {len(text_embedding)}\")\n",
    "print(f\"Here's a sample: {text_embedding[:10]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts - Text generally used as instructions to the model\n",
    "\n",
    "### Prompt\n",
    "\n",
    "What you'll pass to the underlying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIt is missing Tuesday.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "prompt = \"\"\"\n",
    "Today is monday, tomorrow is wednesday.\n",
    "\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "llm(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "\n",
    "An object that helps create prompts based on a combination of user input, other non-static information and a fixed template string.\n",
    "\n",
    "*It's an f-string, just for prompts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "I really want to travel to Rome. What should I do there?\n",
      "\n",
      "Respond in one short sentence.\n",
      "\n",
      "----------------------\n",
      "LLM Output: Explore the Colosseum, the Vatican, and the Trevi Fountain.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location=\"Rome\")\n",
    "\n",
    "print(f\"Final Prompt: {final_prompt}\")\n",
    "print(\"----------------------\")\n",
    "print(f\"LLM Output: {llm(final_prompt)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Selectors\n",
    "\n",
    "An easy way to select from a series of examples that allow you to dinamically place in-context information into your prompt. \n",
    "Often used when your task is nuanced or you have a large list of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\\n\"\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"forest\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from\n",
    "    examples,\n",
    "\n",
    "    # This is the embedding class used to produce embeddings which are used to measure \n",
    "    # similarity between the input and the examples\n",
    "    OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),\n",
    "\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a \n",
    "    # similarity search\n",
    "    FAISS,\n",
    "\n",
    "    # This is the number of examples to produce\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "\n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "\n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "\n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item is usually found in\n",
      "\n",
      "Example Input: driver\n",
      "Example Output: car\n",
      "\n",
      "\n",
      "Example Input: tree\n",
      "Example Output: forest\n",
      "\n",
      "\n",
      "Input: student\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "my_noun = \"student\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' classroom'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "\n",
    "A helpful way to format the ooutput of a model. Usually used for strutured outputs.\n",
    "\n",
    "Two main concepts:\n",
    "\n",
    "**1. Format Instructions** - A autogenerated prompt that tells the LLM how to format its response based off your desired result.\n",
    "\n",
    "**2. Parser** - A methos which will extract your model's text output into a desired structure (usually json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How you'd like your response structured. This is basically a fancy prompt template\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This is a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response,a reformatted response\")\n",
    "]\n",
    "\n",
    "# How you'd like ot parse your putput\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response,a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# See the prompt template you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will be given a poorly formatted string from a user.\n",
      "Reformat it and make sure all the words are spelled corectly.\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This is a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response,a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "welcom to californya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled corectly.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to californya!\")\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"welcom to californya!\",\\n\\t\"good_string\": \"Welcome to California!\"\\n}\\n```'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output = llm(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_parser.parse(llm_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexes - Structuring documents so that LLMs can work with them\n",
    "\n",
    "### Document Loaders\n",
    "\n",
    "Easy ways to import data from other sources. Shared functionality with OpenAI Plugins, specifically retrieval plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 comments\n",
      "Here's a sample: \n",
      "\n",
      "Ozzie_osman 4 months ago  \n",
      "             | next [–] \n",
      "\n",
      "LangChain is awesome. For people not sure what it's doing, large language models (LLMs) are very Ozzie_osman 4 months ago  \n",
      "             | parent | next [–] \n",
      "\n",
      "Also, another library to check out is GPT Index (https://github.com/jerryjliu/gpt_index)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(data)} comments\")\n",
    "print(f\"Here's a sample: \\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.image import UnstructuredImageLoader\n",
    "\n",
    "loader = UnstructuredImageLoader(\"layout-parser-paper-fast.jpg\", mode=\"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gab/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unstructured_inference is not installed, pytesseract is not installed and the text of the PDF is not extractable. To process this file, install unstructured_inference, install pytesseract, or remove copy protection from the PDF.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m~/Aprendizado/LangChain/lang/lib/python3.8/site-packages/langchain/document_loaders/unstructured.py:71\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m     70\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     elements \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_elements()\n\u001b[1;32m     72\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39melements\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     73\u001b[0m         docs: List[Document] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n",
      "File \u001b[0;32m~/Aprendizado/LangChain/lang/lib/python3.8/site-packages/langchain/document_loaders/image.py:13\u001b[0m, in \u001b[0;36mUnstructuredImageLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_elements\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m     11\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39munstructured\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpartition\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m partition_image\n\u001b[0;32m---> 13\u001b[0m     \u001b[39mreturn\u001b[39;00m partition_image(filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munstructured_kwargs)\n",
      "File \u001b[0;32m~/Aprendizado/LangChain/lang/lib/python3.8/site-packages/unstructured/partition/image.py:48\u001b[0m, in \u001b[0;36mpartition_image\u001b[0;34m(filename, file, url, template, token, include_page_breaks, ocr_languages, strategy)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mif\u001b[39;00m template \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     template \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlayout/image\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m \u001b[39mreturn\u001b[39;00m partition_pdf_or_image(\n\u001b[1;32m     49\u001b[0m     filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m     50\u001b[0m     file\u001b[39m=\u001b[39;49mfile,\n\u001b[1;32m     51\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m     52\u001b[0m     template\u001b[39m=\u001b[39;49mtemplate,\n\u001b[1;32m     53\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m     54\u001b[0m     include_page_breaks\u001b[39m=\u001b[39;49minclude_page_breaks,\n\u001b[1;32m     55\u001b[0m     ocr_languages\u001b[39m=\u001b[39;49mocr_languages,\n\u001b[1;32m     56\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m     57\u001b[0m )\n",
      "File \u001b[0;32m~/Aprendizado/LangChain/lang/lib/python3.8/site-packages/unstructured/partition/pdf.py:112\u001b[0m, in \u001b[0;36mpartition_pdf_or_image\u001b[0;34m(filename, file, url, template, token, is_image, include_page_breaks, strategy, infer_table_structure, ocr_languages)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m route_args[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlayout\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    110\u001b[0m     out_template \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m strategy \u001b[39m=\u001b[39m determine_pdf_or_image_strategy(\n\u001b[1;32m    113\u001b[0m     strategy,\n\u001b[1;32m    114\u001b[0m     filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m    115\u001b[0m     file\u001b[39m=\u001b[39;49mfile,\n\u001b[1;32m    116\u001b[0m     is_image\u001b[39m=\u001b[39;49mis_image,\n\u001b[1;32m    117\u001b[0m     infer_table_structure\u001b[39m=\u001b[39;49minfer_table_structure,\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m strategy \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhi_res\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    121\u001b[0m     \u001b[39m# NOTE(robinson): Catches a UserWarning that occurs when detectron is called\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n",
      "File \u001b[0;32m~/Aprendizado/LangChain/lang/lib/python3.8/site-packages/unstructured/partition/strategies.py:98\u001b[0m, in \u001b[0;36mdetermine_pdf_or_image_strategy\u001b[0;34m(strategy, filename, file, is_image, infer_table_structure)\u001b[0m\n\u001b[1;32m     93\u001b[0m     file\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m     96\u001b[0m     [\u001b[39mnot\u001b[39;00m unstructured_inference_installed, \u001b[39mnot\u001b[39;00m pytesseract_installed, \u001b[39mnot\u001b[39;00m pdf_text_extractable],\n\u001b[1;32m     97\u001b[0m ):\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     99\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39munstructured_inference is not installed, pytesseract is not installed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mand the text of the PDF is not extractable. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo process this file, install unstructured_inference, install pytesseract, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor remove copy protection from the PDF.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m strategy \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfast\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m pdf_text_extractable:\n\u001b[1;32m    106\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    107\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPDF text is not extractable. Cannot use the fast partitioning \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstrategy. Falling back to partitioning with the ocr_only strategy.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: unstructured_inference is not installed, pytesseract is not installed and the text of the PDF is not extractable. To process this file, install unstructured_inference, install pytesseract, or remove copy protection from the PDF."
     ]
    }
   ],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "081b0ec414cf34c520c2ad73a9e4b2cb7962fffb5b4b6d89b1107f288d252085"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
